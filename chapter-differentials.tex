%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                 %%%%%
%%%%%   Derivations   %%%%%
%%%%%                 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Differential Forms}
\label{chap_differentials}

In a typical undergraduate mathematics curriculum,
derivatives and differential forms appear in the study of real- or complex-valued functions.
In real and complex analysis courses,
it is seen that $\bb R$ and $\bb C$ are complete with respect to their standard metrics.
Concepts such as limits, convergence, and derivatives are defined with respect to these metrics.
In this thesis, we wish to speak of derivatives and differentials of functions that are not real-valued,
but rather members of a polynomial ring $K[x,y]$ over a finite field or a quotient of that polynomial ring.
It is not so clear anymore what is meant by the differential of a polynomial in these discrete spaces.

In this chapter, we define (K\"ahler) differentials of functions purely algebraically
in a sufficient generality as to cover differentials of polynomials in $A = R[x_1, \ldots, x_n]$,
multivariate polynomials with coefficients in a commutative ring with identity $R$.
We will see how to extend this definition to differentials on functions in other spaces constructed from $A$,
such that its field of fractions, quotients, and localizations.
Along the way, we give a natural definition of the formal derivative and formal partial derivative.

The contents of this chapter come mostly from a combination of the books
\cite{eisenbud95}, \cite{eisenbud00}, \cite{goldschmidt03}, and \cite{stichtenoth09}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                 %%%%%
%%%%%   Derivations   %%%%%
%%%%%                 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Derivations}
\begin{definition}
  Let $R$ be a \note{commutative?} ring, $A$ an $R$-algebra, and $M$ an $A$-module.
  A map $\delta : A \to M$ is called a \defn{derivation} (from $A$ to $M$)
  if it is $R$-linear and satisfies the product rule
  (also called the Leibniz rule):
    \[ \delta(ab) = \delta(a)b + a\delta(b). \]
\end{definition}

Some authors do not require a derivation to be $R$-linear,
and they distinguish between derivations and $R$-linear derivations.
We will assume all derivations are $R$-linear.

As the name suggests, many familiar properties of the derivative from calculus
are an immediate consequence of this definition.
The following are easily verified.
\begin{proposition}
  \label{prop_derivation}
  A derivation satisfies the following properties.
  \begin{enumerate}[label=(\roman*)]
    \item If $A$ is unital, then $\delta(1) = 0$.
    \item If $A$ is unital, then $\delta(r) = 0$ for all $r \in R$.
    \item If $A$ is commutative, then $\delta(x^2) = 2x\delta(x)$.
    \item If $A$ is commutative, then for all integers $n > 0$, $\delta(x^n) = nx^{n-1}\delta(x)$.
    \item If $x$ is a unit, then $\delta(x\inv) = -x^{-2}\delta(x)$.
    \item If $x$ is a unit, then for all $n \in \bb Z$, $\delta(x^n) = nx^{n-1}\delta(x)$.
    \item If $y$ is a unit, then $\delta(xy\inv) = (\delta(x)y - x\delta(y))y^{-2}$.
  \end{enumerate}
\end{proposition}
\begin{comment}
\begin{proof}
  \begin{enumerate}[label=(\roman*)]
    \item
      We have
        \[ D(1) = D(1 \cdot 1) = D(1) \cdot 1 + 1 \cdot D(1) = D(1) + D(1) \]
      which implies $D(1) = 0$.
    \item
      Follows from (i) by $R$-linearity.
    \item
        \[ D(x^2) = D(x)x + xD(x) = xD(x) + xD(x) = 2xD(x). \]
    \item
      Follows from (iii) by induction.
      Suppose $D(x^k) = kx^{k-1}D(x)$ for some $k > 0$. Then
      \begin{align*}
        D(x^{k+1})
          &= D(x^k \cdot x) \\
          &= D(x^k)x + x^kD(x) \\
          &= kx^{k-1}D(x)x + x^kD(x) \\
          &= kx^kD(x) + x^kD(x) \\
          &= (k + 1)x^kD(x).
      \end{align*}
    \item
      By part (i) and the product rule, we have
        \[ 0 = D(1) = D(xx\inv) = D(x)x\inv + xD(x\inv). \]
      This implies
        \[ xD(x\inv) = -D(x)x\inv, \]
      hence
        \[ D(x\inv) = -x^{-2}D(x). \]
    \item
      The case where $n \geq -1$ is handled by parts (i), (iv), and (v).
      The rest follows by induction.
      Suppose $D(x^k) = kx^{k-1}D(x)$ for some $k \leq -1$. Then
      \begin{align*}
        D(x^{k-1})
          &= D(x^kx\inv) \\
          &= D(x^k)x\inv + x^kD(x\inv) \\
          &= kx^{k-1}D(x)x\inv - x^kx^{-2}D(x) \\
          &= kx^{k-2}D(x) - x^{k-2}D(x) \\
          &= (k - 1)x^{k-2}D(x).
      \end{align*}
    \item
      Immediate from the product rule and part (v).
      \begin{align*}
        D(xy\inv)
          &= D(x)y\inv + xD(y\inv) \\
          &= D(x)y\inv - xy^{-2}D(y) \\
          &= (D(x)y - xD(y))y^{-2}.
      \end{align*}
  \end{enumerate}
\end{proof}
\end{comment}

We can define the sum of two derivations $\delta_1$ and $\delta_2$ by
  \[ (\delta_1 + \delta_2)(x) = \delta_1(x) + \delta_2(x) \]
and scalar multiplication of a derivation $\delta$ by a ring element $r$ by
  \[ (r\delta)(x) = r\delta(x). \]
Under these operations, the set of derivations from $A$ to $M$ becomes an $R$-module,
denoted by $\Der_R(A,M)$.
This is the \defn{module of derivations from $A$ to $M$}.
In the case where $M = A$, this may be denoted by $\Der_R(A)$.

Property (v) of Proposition \ref{prop_derivation} shows that if $x$ is a unit,
then $\delta(x\inv)$ is determined by $\delta(x)$.
This suggests that there is a relationship between derivations from an algebra $A$
and derivations from the field of fractions of $A$,
although note that $A$ must not have any zero divisors in order for its field of fractions to be constructed.

\begin{proposition}
  \label{prop_derivation_unique_extension}
  Let $A$ be an $R$-algebra with no zero divisors and let $\delta \in \Der_R(A,M)$.
  Let $B = \Frac A$, the field of fractions of $A$.
  There is a unique $\delta' \in \Der_R(B, M)$ whose restriction to $A$ is $\delta'|_A = \delta$.
\end{proposition}
That is to say that if $\delta$ is a derivation from $A$ to $M$,
it extends uniquely to a derivation from $\Frac A$.
\begin{proof}
  If $A = B$, we are done.
  So suppose instead there is a $b \in B$ such that $b\in A$ but $b\inv \not \in A$.
  Suppose $\delta_1, \delta_2 \in \Der_R(B, M)$ are such that $\delta_1|_{A} = \delta = \delta_2|_{A}$.
  Then $\delta_1(b) = \delta(b) = \delta_2(b)$ and
    \[ \delta_1(b\inv) = -b^{-2}\delta_1(b) = -b^{-2}\delta_2(b) = \delta_2(b\inv). \]
  It follows that $\delta_1(ab\inv) = \delta_2(ab\inv)$ for all $ab\inv \in B$.
\end{proof}

In order to understand how a derivation acts on $\Frac A$, it is enough to know how it acts on $A$.
In the case of a derivation over the field $R(x_1, \ldots, x_n)$,
it is enough to know how it acts on the polynomial ring $R[x_1, \ldots, x_n]$.
In the univariate case, we can say even more.
The behaviour of a derivation $\delta: R(x) \to M$ is entirely determined by the value of $\delta(x)$.

\begin{proposition}
  \label{prop_derivation_unique_x}
  Let $R(x)$ be the ring of rational functions over a ring $R$ in a single variable $x$.
  Let $\delta_1, \delta_2 \in \Der_R(R(x))$.
  If $\delta_1(x) = \delta_2(x)$, then $\delta_1 = \delta_2$.
\end{proposition}
\begin{proof}
  For all $r \in R$, we have $\delta_1(r) = \delta_2(r) = 0$.
  For all $f \in R[x]$, we have $\delta_1(f) = \delta_2(f)$ by $R$-linearity.
  Now $\delta_1$ and $\delta_2$ agree on all of $R[x]$.
  By Proposition \ref{prop_derivation_unique_extension}, they must agree on all of $R(x)$.
\end{proof}

For a (unital) ring $R$, the \defn{formal derivative} on $R(x)$
is the unique derivation $D_x \in \Der_R(R(x))$ satisfying $D_x(x) = 1$.
The formal derivative of a function $f$ is often denoted $f' := D_x(f)$,
although this convention will not be adopted here ---
beyond this paragraph, this thesis has no use for derivations on univariate polynomials.
The formal derivative behaves exactly as one might expect.
\begin{example}
  Let $f = 3x^3 + 6x^2 + 1 \in \bb Z[x]$. Then
  \begin{align*}
    D_x(f) 
      &= 3D_x(x^3) + 6D_x(x^2) + D_x(1) & R\text{-linearity} \\
      &= 3 \cdot 3 x^2 D_x(x) + 6 \cdot 2 x D_x(x) + 0 & \text{Proposition \ref{prop_derivation}}\\
      &= 9 x^2 + 12 x & D_x(x) = 1.
  \end{align*}
\end{example}

\begin{comment}
Up to this point, we have discussed derivations without showing that such a thing even exists.
The following proposition establishes the existence of derivations by exhibiting one that should be familiar.

\begin{proposition}
  \label{prop_derivation_formal_derivative}
  Let $R(x)$ be the ring of rational functions over a ring $R$ in a single variable $x$.
  There is a unique derivation $D_x : R(x) \to R(x)$ satisfying $D_x(x) = 1$.
\end{proposition}
Given a rational function $f(x)$, the function $D_x(f(x))$ is the \defn{formal derivative} of $f(x)$, often written $f'(x)$.
\begin{proof}
  We need only show existence of a derivation on $R[x]$ with the desired property.
  By propositions \ref{prop_derivation_unique_extension} and \ref{prop_derivation_unique_x},
  this derivation is unique and extends uniquely to a derivation on $R(x)$ with the desired property.
  
  We set $D_x(x) = 1$ and extend by linearity to get its action on all of $R[x]$.
  If $f(x) = \sum f_ix^i \in R[x]$, then
    \[ D(f(x)) = \sum if_ix^{i-1}D(x) = \sum if_ix^{i-1}. \]
  We must show that this satisfies the product rule.
  Let $g(x) = \sum g_jx^j$. Then for all $f(x), g(x) \in R[x]$,
  
  \begin{align*}
    D_x(f(x)g(x))
      &= D_x \left( \left( \sum_{i=0}^m f_ix^i \right) \left( \sum_{j=0}^n g_jx^j \right) \right) \\
      &= D_x \left( \sum_{i=0}^m \sum_{j=0}^n f_ix^i g_jx^j \right) \\
      &= D_x \left( \sum_{i=0}^m \sum_{j=0}^n f_ig_jx^{i+j} \right) \\
      &= \sum_{i=0}^m \sum_{j=0}^n f_ig_jD_x(x^{i+j}) \\
      &= \sum_{i=0}^m \sum_{j=0}^n (i+j)f_ig_jx^{i+j-1} \\
      &= \sum_{i=0}^m \sum_{j=0}^n (if_ig_jx^{i+j-1} + jf_ig_jx^{i+j-1}) \\
      &= \sum_{i=0}^m \sum_{j=0}^n if_ig_jx^{i+j-1} + \sum_{i=0}^m \sum_{j=0}^n jf_ig_jx^{i+j-1} \\
      &= \left( \sum_{i=0}^m if_ix^{i-1} \right) \left( \sum_{j=0}^n g_jx^j \right) + \left( \sum_{i=0}^m f_ix^i \right) \left( \sum_{j=0}^n jg_jx^{j-1} \right) \\
      &= D_x(f(x)) g(x) + f(x)D(g(x)).
  \end{align*}
\end{proof}
\end{comment}

Now consider the $R$-algebra $A = R(x_1, \ldots, x_n)$, rational functions in $n$ variables.
For each $1 \leq i \leq r$,
we can set $S_i = R(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n)$, so that $A = S_i(x_i)$.
We have merely realized $A$ as the algebra of rational functions in one variable $x_i$ and coefficients in $S_i$,
which is the algebra of rational functions in the other $n - 1$ variables.
Then Proposition \ref{prop_derivation_unique_x} says that
there is a unique derivation $D_{x_i} \in \Der_{S_i}(A)$ satisfying $D_{x_i}(x_i) = 1$.
This derivation $D_{x_i}$ is also a member of $\Der_R(A)$ and its action on the other indeterminates of $A$ is
\begin{align*}
  D_{x_i}(x_j) &= \begin{cases} 1 & i = j \\ 0 & i \neq j \end{cases}.
\end{align*}
The requirement that $D_{x_i}(x_j) = 0$ when $i \neq j$ is a consequence of $S_i$-linearity
by $D_{x_i}$'s membership in $\Der_{S_i}(A)$.
If $f \in A$, then $f_{x_i} := D_{x_i}(f)$ is the \defn{formal partial derivative} with respect to $x_i$.

\begin{example}
  In the case of $A = \bb Z[x,y]$, we have derivations $D_x$ and $D_y$ with
  $D_x(x) = 1 = D_y(y)$ and $D_x(y) = 0 = D_y(x)$.
  Let $f = x^2 + xy + y^3 \in A$.
  Then $f_x = D_x(f) = 2x + y$ and $f_y = D_y(f) = x + 3y^2$.
\end{example}

\begin{comment}
\begin{proposition}
  Let $D \in \Der_K(A)$ be a derivation.
  Let $I$ be an ideal of $A$.
  Then $D$ induces a derivation $D^* \in \Der_K(A/I)$ defined by
    \[ D^*([a]) = [D(a)]. \]
\end{proposition}
\begin{proof}
  For each $a \in A$, denote the equivalence class containing $a$ in $A/I$ by $\bar a$.
  We show first that this map is well-defined.
  Suppose $\bar a = \bar b$.
  Then $\bar{a-b} = \bar 0$ and
    \[ D^*(\bar a) = D^*(\bar a - \bar{a-b}) = D^*(\bar{a-a+b}) = D^*(\bar b). \]
  We must also show that $D^*(\bar k \cdot \bar a) = \bar k \cdot D^*(\bar a)$
  and $D^*(\bar a \cdot \bar b) = D^*(\bar a) \cdot \bar b + \bar a \cdot D^*(\bar b)$.
    \[ D^*(\bar k \cdot \bar a) = D^*(\bar{ka}) = \bar{D(ka)} = \bar{kD(a)} = \bar k \cdot \bar{D(a)} = \bar k \cdot D^*(\bar a) \]
  \begin{align*}
    D^*(\bar a \cdot \bar b)
      &= D^*(\bar{ab})
       = \bar{D(ab)}
       = \bar{D(a)b + aD(b)} \\
       &= \bar{D(a)} \cdot \bar b + \bar a \cdot \bar{D(b)}
       = D^*(\bar a) \cdot \bar b + \bar a \cdot D^*(\bar b)
  \end{align*}
\end{proof}
With this we can extend formal derivatives of functions in $K[x,y]$ and $K(x,y)$ to formal derivatives of functions in $K[C]$ and $K(C)$.
\end{comment}

We may compose derivations with a morphisms of algebras to obtain new derivations.

\begin{proposition}
  \label{prop_precompose_derivation}
  Let $f \in \Hom_R(A,B)$ be a morphism of $R$-algebras and let $\delta \in \Der_R(B,M)$ be an $R$-linear derivation.
  \[ \begin{tikzcd} A \arrow{r}{f} & B \arrow{r}{\delta} & M \end{tikzcd} \]
  Then $\delta \circ f \in \Der_R(A,M)$.
\end{proposition}
\begin{proof}
  While $M$ is a $B$-module, it becomes an $A$-module under the $A$-action
    \[ A \times M \to M : (a, m) \mapsto f(a)m. \]
  Both $f$ and $\delta$ are $R$-linear, so $\delta \circ f$ is also $R$-linear.
  As for the product rule, let $a, b \in A$. Then
  \begin{align*}
    (\delta \circ f)(ab)
      &= \delta(f(ab)) \\
      &= \delta(f(a)f(b)) \\
      &= \delta(f(a))f(b) + f(a)\delta(f(b)) \\
      &= (\delta \circ f(a))f(b) + f(a)(\delta \circ f(b)) \\
      &= (b, \delta \circ f(a)) + (a, \delta \circ f(b)). \qedhere
  \end{align*}
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                          %%%%%
%%%%%   Universal Derivation   %%%%%
%%%%%                          %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{K\"ahler Differentials}
\label{sec_kahler_differentials}

In this section, we briefly present a construction of K\"ahler differentials and some important properties.
For more detail, as well as alternative constructions,
see \cite{eisenbud95}, \cite{eisenbud00}, \cite{goldschmidt03}, or \cite{stichtenoth09}.

Let $A$ be an $R$-algebra.
The \defn{module of K\"ahler differentials} of $A$ over $R$,
denoted by $\Omega_{A/R}$,
is the $A$-module generated by the set $\{ d(a) ~|~ a \in A \}$,
where $d(a)$ is merely a symbol,
modulo the relations
\begin{align*}
  d(ab) &= d(a)b + ad(b) \\
  d(ra + sb) &= rd(a) + sd(b)
\end{align*}
for all $r, s \in R$ and $a, b \in A$.
The elements of $\Omega_{A/R}$ are called \defn{differential forms} or \defn{K\"ahler differentials}.
We will write $d(a)$ as simply $da$, though we will not write $d(ab)$ as $dab$, as this may be confused with $d(a)b$.
The map
  \[ d_A : A \to \Omega_{A/R} : a \mapsto d(a) \]
is an $R$-linear derivation, called the \defn{universal derivation}.
It is universal in the following sense.
If $\delta : A \to M$ is another $R$-linear derivation from $A$,
then there is a unique morphism of $R$-modules $\phi : \Omega_{A/R} \to M$ such that
\[ \begin{tikzcd}
  A \arrow{r}{d_A} \arrow[swap]{dr}{\delta} & \Omega_{A/R} \arrow[dashed]{d}{!\exists \phi} \\ & M
\end{tikzcd} \]
commutes.
This map is defined by $\phi : da \mapsto \delta(a)$.
We will omit the subscript and write $d$ in place of $d_A$,
except in contexts where there are multiple $d$'s and we need to distinguish them by their domains.

\begin{comment}
\begin{proposition}
  A morphism $\phi : A \to B$ of $R$-algebras induces
  a morphism $\psi : \Omega_{A/R} \to \Omega_{B/R}$ of $A$-modules.
\end{proposition}
\begin{proof}
  Let $\phi : A \to B$ be a morphism of $R$-algebras.
  We have the following maps,
  \[ \begin{tikzcd}
      A \arrow{r}{d_A} \arrow[swap]{d}{\phi} & \Omega_{A/R} \\ B \arrow[swap]{r}{d_B} & \Omega_{B/R}.
    \end{tikzcd} \]
  By Proposition \ref{prop_precompose_derivation},
  the composition $d_B \circ \phi : A \to \Omega_{B/R}$ is an $R$-linear derivation.
  By the universal property of the universal derivation,
  there is a unique map $\psi : \Omega_{A/R} \to \Omega_{B/R}$ such that
  \[ \begin{tikzcd}
      A \arrow{r}{d_A} \arrow[swap]{dr}{d_B \circ \phi} & \Omega_{A/R} \arrow[dashed]{d}{\psi} \\ & \Omega_{B/R}
    \end{tikzcd} \]
  commutes.
\end{proof}
The induced map sends $d_Aa \mapsto d_B(\phi(a))$.
\end{comment}

Of particular interest to us in this thesis is the case where $A = R[x_1, \ldots, x_n]$ is a polynomial ring.
In this case, $\Omega_{A/R}$ is the free $A$-module generated by the symbols $dx_1, \ldots, dx_n$.
\begin{proposition}
  \label{prop_differential_module_of_polynomial_ring}
  Let $A = R[x_1, \ldots, x_n]$. Then
  \[ \Omega_{A/R} \cong \bigoplus_{i=1}^n Adx_i. \]
\end{proposition}
\begin{comment}
\begin{corollary}
  Let $A = R[x_1, \ldots, x_n]$, let $I$ be an ideal of $A$, and let $B = A/I$.
  Then $B$ is an $R$-algebra and
  \[ B \otimes_{R} \Omega_{A/R} \cong \bigoplus_{i=1}^n Bdx_i. \]
\end{corollary}
\begin{proof}
  \note{Extension of scalars.}
\end{proof}
Here $\otimes_R$ denotes the tensor product of algebra.
We will use this corollary in Chapter \ref{chap_curves}, after defining the coordinate ring of a curve,
when relating differentials forms of a polynomial ring to those of a coordinate ring.
Since $\Omega_{A/R}$ is a direct sum of $R$-algebras $Adx_i$,
there is a family of natural projection maps 
  \[ \pi_i : \Omega_{A/R} \to Adx_i : \sum f_jdx_j \mapsto f_idx_i. \]
We can compose the maps
  \[ \begin{tikzcd}
    A \arrow{r}{d} & \Omega_{A/R} \arrow{r}{\pi_i} & Adx_i \arrow{r}{} & A
  \end{tikzcd} \]
where the right-most map sends $dx_i \mapsto 1$.
The composition of the three maps is the formal partial derivative with respect to $x_i$.
\end{comment}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                                %%%%%
%%%%%   Differential Forms in K[C]   %%%%%
%%%%%                                %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Differential Forms in $K[C]$}
\label{sec_differentials_in_coordinate_ring}

In Section \ref{sec_kahler_differentials}, we defined $\Omega_{A/R}$,
the module of K\"ahler differentials on an $R$-algebra $A$.
The coordinate ring is a $K$-algebra, so let us describe the structure of $\Omega_{K[C]/K}$.
To do so, we will make use of the following proposition.

\begin{proposition}
  \label{prop_conormal_sequence}
  Let $\pi : A \to B$ be an epimorphism of $R$-algebras.
  Let $I = \ker \pi$.
  There is an exact sequence of $B$-modules
    \[ I/I^2 \xrightarrow{d} B \tensor A \Omega_{A/R} \xrightarrow{D\pi} \Omega_{B/R} \to 0 \]
  where $d : [f] \mapsto 1 \otimes df$ and $D\pi : b \otimes da \mapsto b(da)$.
\end{proposition}
\begin{proof}
  Proposition 16.3 in \cite{eisenbud95}.
\end{proof}

This proposition makes use of the tensor product of modules.
The tensor product will not be defined or discussed in this thesis;
one may consult \cite{dummit04}, \cite{eisenbud95}, or \cite{hungerford} for more on that topic.
We will take advantage of other results to turn the tensor product into a direct sum of modules.

To understand the structure of $\Omega_{K[C]/K}$,
we begin by noting that the canonical quotient map $q : K[x,y] \to K[C]$ is an epimorphism of $K$-algebras
whose kernel is $\ker q = \pid F$, the ideal of $K[x,y]$ generated by the defining polynomial of the curve.
Proposition \ref{prop_conormal_sequence} therefore applies, telling us that there is an exact sequence
\begin{center}
\begin{tikzcd}
  \frac{\pid{F}}{\pid{F^2}} \arrow{r}{d} &
  K[C] \tensor {K[x,y]} \Omega_{K[x,y]/K} \arrow{r}{Dq} &
  \Omega_{K[C]/K} \arrow{r}{} &
  0.
\end{tikzcd}
\end{center}
Using a property of exact sequences (see remarks on page 176 of \cite{hungerford}), 
\[ \Omega_{K[C]/K} \cong \frac {K[C] \tensor {K[x,y]} \Omega_{K[x,y]/K}} {\im d}. \]
Next applying Proposition \ref{prop_differential_module_of_polynomial_ring},
\[ \Omega_{K[C]/K} \cong \frac {K[C] \tensor {K[x,y]} \Omega_{K[x,y]/K}} {\im d}
                   \cong \frac {K[C] \tensor {K[x,y]} (K[x,y]dx \oplus K[x,y]dy)} {\im d}. \]
The tensor product distributes over addition (Theorem IV.5.9 \cite{hungerford}), giving
\[ \Omega_{K[C]/K} \cong \frac {\left(K[C] \tensor {K[x,y]} K[x,y]dx\right) \oplus
                                \left(K[C] \tensor {K[x,y]} K[x,y]dy\right)} {\im d}. \]
A basic property of the tensor product is that a $R$-module $A$ tensored with its ring $R$ is isomorphic to itself,
i.e. $A \tensor R R \cong A$ (Theorem IV.5.7 \cite{hungerford}), hence
\[ \Omega_{K[C]/K} \cong \frac {K[C]dx \oplus K[C]dy} {\im d}. \]

To determine the image of $d$, it is enough to know the image of the element $F \in \pid F / \pid{F^2}$.
The element $F \in F \in \pid F / \pid{F^2}$ maps to $1 \otimes dF \in K[C] \otimes \Omega_{K[x,y]/K}$.
Following the chain of isomorphisms we just produced, this then maps to $F_xdx + F_ydy \in K[C]dx \oplus K[C]dy$.
Thus,
\[ \Omega_{K[C]/K} \cong \frac {K[C]dx \oplus K[C]dy} {\pid{F_xdx + F_ydy}}. \]
The module $\Omega_{K[C]/K}$ of differentials on $K[C]$ is generated by $dx$ and $dy$,
modulo the equivalence relation $F_xdx \equiv -Fydy$.
This relation between $dx$ and $dy$ means that it is therefore a rank 1 $K[C]$-module, generated by a single element.
\begin{proposition}
  \label{prop_differential_generator}
  There is a generator $dz$ for $\Omega_{K[C]/K}$ with the properties
  \begin{align*}
    dx &\equiv C_ydz \\
    dy &\equiv -C_xdz.
  \end{align*}
\end{proposition}
\begin{proof}
  See Lemma 5.1 in \cite{salem07}.
\end{proof}
In fact, there are many possible generators for $\Omega_{K[C]/K}$.
Another choice of generator allowing for more efficient doubling of typical divisors
will be presented in Chapter \ref{chap_doubling}.
The generator in Proposition \ref{prop_differential_generator} is a natural choice
that is useful for some proofs, and will be used in some atypical cases of divisor doubling.

The differential of a function $f \in K[C]$ with respect to this generator $dz$ is
\[ df = (f_xC_y - f_yC_x)dz. \]
If $I$ is an ideal of $K[C]$,
we will say by abuse of notation that $df \in I$ if $f_xC_y - f_yC_x \in I$.
Equivalently, we will say that $df \in I$ if $df$ vanishes modulo $I$.

Now let $\cal O_{\frak p}$ be the local ring at a prime ideal $\frak p$ of $K[C]$.
Then there is a map $d_{\cal O_{\frak p}} : \cal O_{\frak p} \to \Omega_{\cal O_{\frak p}/K}$.
The action of this map is inherited from the map $d_{K[C]} : K[C] \to \Omega_{K[C]/K}$.
That is, the differential of a function $f/g \in \cal O_{\frak p}$ is
\[ d_{\cal O_{\frak p}} \left( \frac f g \right) = \frac{d_{K[C]}(f)g - fd_{K[C]}(g)}{g^2}
 = \frac{(f_xC_y - f_yC_x)g - f(g_xC_y - g_yC_x)}{g^2} dz . \]
As noted earlier in this chapter, for readability, we will omit the subscripts on $d$
when it is clear in context which differential map is meant.

\begin{lemma}
  \label{lem_differential_of_uniformizer}
  Let $\frak p$ be a non-zero prime ideal of $\bar K[C]$.
  Let $u$ be a uniformizer for $\frak m_{\frak p}$, the maximal ideal of $\cal O_{\frak p}$.
  Then $du \not \in \frak m_{\frak p}$.
\end{lemma}
\begin{proof}
  Let $\frak p = \pid{x - x_0, y - y_0}$.
  Since $C$ is non-singular, the partial derivatives of $C$ are not simultaneously zero at $P = (x_0, y_0)$,
  so suppose without loss of generality that $C_y(x_0, y_0) \neq 0$.
  Then the tangent line to $C$ at $P$ is non-vertical and $u = x - x_0$ is a uniformizer for $\frak m_{\frak p}$.
  Then $du = dx = C_y \not\in \frak m_{\frak p}$.
\end{proof}
\begin{theorem}
  \label{thm_differential_increases_order}
  Let $\frak p$ be a non-zero prime ideal of $K[C]$ and $f$ a polynomial in $K[C]$.
  Suppose $\nu_{\frak p}(f) \geq n$ for some non-negative integer $n$. Then
  \[ \nu_{\frak p}(f) \geq n + 1 \iff \nu_{\frak p}(df) \geq n. \]
\end{theorem}
\begin{proof}
%\frak m_{\frak p}
  Let $u$ be a uniformizer for $\frak m_{\frak p}$.
  Then $f = au^n$ for some $a \in K(C)^*$, and
  \[ df = d(au^n) = u^nda + nau^{n-1}du. \]
  Now $u^nda \in \frak m_{\frak p}^n$,
  so $df \in \frak m_{\frak p}^n$ if and only if $nau^{n-1}du \in \frak m_{\frak p}^n$.
  But $n, du \not\in \frak m_{\frak p}$,
  so this is true if and only if $au^{n-1} \in \frak m_{\frak p}^n$. Hence
  \[ df \in \frak m_{\frak p}^n
    \iff au^{n-1} \in \frak m_{\frak p}^n
    \iff au^n \in \frak m_{\frak p}^{n+1}
    \iff f \in \frak m_{\frak p}^{n+1}. \qedhere \]
\end{proof}
\begin{comment}
\begin{theorem}
  Let $A = K[x_1, \ldots, x_n]$ and let $I$ be an ideal of $A$.
  If $f \in I^2$, then $f \equiv 0$ and $df \equiv 0$ modulo $I$.
\end{theorem}
\begin{proof}
  Let $f \in I^2$.
  Then $f \in I$, so $f \equiv 0 \pmod I$.
  As for its differential, we have that $f$ is generated by a Gr\"obner basis {$g_i$}
  \[ f = \sum a_{i,j}g_ig_j, \]
  so
  \begin{align*}
    df &= d \left( \sum_{\substack{1 \leq i \leq j \leq m}} a_{i,j}g_ig_j \right) \\
       &= \sum_{\substack{1 \leq i \leq j \leq m}} d \left( a_{i,j}g_ig_j \right) \\
       &= \sum_{\substack{1 \leq i \leq j \leq m}} \left( d(a_{i,j})g_ig_j + a_{i,j}d(g_i)g_j + a_{i,j}g_id(g_j) \right) \\
       &\equiv 0 \pmod I
  \end{align*}
\end{proof}

The converse is not true in general.
A simple counterexample is to let $A = \bb F_2[x]$, let $f = x^2$, and let $I = \pid f$.
Then $f \equiv 0 \pmod I$ and $df = 2xdx \equiv 0 \pmod I$.
However, $f \not\in I^2 = \pid{x^4}$.

It is true under additional assumptions.

\begin{theorem}
  Let $A = K[x_1, \ldots, x_n]$ and let $I$ be an ideal of $A$.
  Let $f \in A$ be a polynomial whose formal partial derivatives do not all vanish.
  If $f, df \equiv 0 \pmod I$, then $f \in I^2$.
\end{theorem}
\begin{proof}
  We prove the contrapositive.
  Suppose $f \not\in I^2$.
  If $f \not\equiv 0 \pmod I$, we are done, so suppose $f \equiv 0 \pmod I$ (i.e. $f \in I$).
  We wish to show that $df \not\equiv 0 \pmod I$.

  By Theorem \ref{thm_groebner_basis_remainder}, we can write $f$ as
  \[ f = g + r \]
  where $g \in I^2$ and $r \not\in \LT(I^2)$. Furthermore, $0 \neq r \in I$.
  Since $r \not\in \LT(I^2)$, $D_{x_k}(r) \not\in \LT(I^2)$ for any $1 \leq k \leq n$.
  \begin{align*}
    df &= dg + dr \\
       &\equiv dr \pmod I \\
       &= \sum D_{x_i}(r)dx_i
  \end{align*}
  We must argue now that for each summand $D_{x_i}(r)dx_i$ is non-zero modulo $I$.
  
  Suppose that $D_{x_k}(r) \equiv 0$ for some $1 \leq k \leq n$.
  \begin{align*}
    & D_{x_k}(r) \equiv 0 \pmod I \\
    \implies & D_{x_k}(r) \in I \\
    \implies & \LT(D_{x_k}(r)) \in \LT(I) \\
    \implies & \LT(D_{x_k}(r)) \in \LT(I)\LT(I) \\
    \implies & \LT(D_{x_k}(r)) \in \LT(I^2).
  \end{align*}
  However, as noted earlier, no term in $D_{x_k}(r)$ is in $\LT(I^2)$.
  \note{(Wording.)}
\end{proof}

\begin{proof}
  We prove the contrapositive.
  Suppose $f \not\in I^2$.
  If $f \not\equiv 0 \pmod I$, we are done, so suppose $f \equiv 0 \pmod I$ (i.e. $f \in I$).
  We wish to show that $df \not\equiv 0 \pmod I$.

  By Theorem \ref{thm_groebner_basis_remainder}, we can write $f$ as
  \[ f = g + r \]
  where $g \in I^2$ and $r \not\in \LT(I^2)$.
  Taking the differential of $f$,
  \begin{align*}
    df &= \sum_{i=1}^n D_{x_i}(f)dx_i.% \\
%       &= \sum_{i=1}^n D_{x_i}(g + r)dx_i \\
  \end{align*}
  We must show that one of $df$'s summands is non-zero modulo $I$.
  Since $f$ has a non-zero formal partial derivative, let $k$ be such that $D_{x_k}(f) \neq 0$.
  Then
  \begin{align*}
    D_{x_k}(f)
      &= D_{x_k}(g + r) \\
      &= D_{x_k}(g) + D_{x_k}(r) \\
      &\equiv D_{x_k}(r) \pmod I.
  \end{align*}
  Now suppose $D_{x_k}(r) \equiv 0 \pmod I$. Then
  \begin{align*}
    & D_{x_k}(r) \equiv 0 \pmod I \\
    \implies & D_{x_k}(r) \in I \\
    \implies & \LT(D_{x_k}(r)) \in \LT(I) \\
    \implies & \LT(D_{x_k}(r)) \in \LT(I)\LT(I) \\
    \implies & \LT(D_{x_k}(r)) \in \LT(I^2).
  \end{align*}
  %However, as noted earlier, no term in $D_{x_k}(r)$ is in $\LT(I^2)$.
  %\note{(Wording.)}
\end{proof}

\begin{proof}
  \note{(Supposing $I = \sqrt I$.)}
  We prove the contrapositive.
  Suppose $f \not\in I^2$.
  If $f \not\equiv 0 \pmod I$, we are done, so suppose $f \equiv 0 \pmod I$ (i.e. $f \in I$).
  We wish to show that $df \not\equiv 0 \pmod I$.

  By Theorem \ref{thm_groebner_basis_remainder}, we can write $f$ as
  \[ f = g + r \]
  where $g \in I^2$ and $r \not\in \LT(I^2)$.
  Taking the differential of $f$,
  \begin{align*}
    df &= dg + dr \\
       &\equiv dr \pmod I \\
       &= d\left(\sum_{i=1}^m a_ig_i \right) \\
       &\equiv \sum_{i=1}^m a_id(g_i) \pmod I.
  \end{align*}
\end{proof}



\begin{theorem}
  Let $\frak p$ be a prime ideal of $K[C]$.
  Let $f \in \frak p$.
  Then $f \in \frak p^2$ if and only if $df \in \frak p$.
\end{theorem}
\begin{proof}
  Let $r = \ord_{\frak p}f$. 
  Then $f$ may be written
  \[ f = \sum_{i=1}^r a_iu^i,\]
  where $u$ is a uniformizer of $\frak m_{\frak p}$ and $a_i \not\in \frak m_{\frak p}$. Then
  \begin{align*}
    df &= \sum_{i=1}^r d(a_iu^i) \\
       &= \sum_{i=1}^r (u^ida_i + a_id(u^i)) \\
       &= \sum_{i=1}^r (u^ida_i + ia_iu^{i-1}du) \\
       &= \sum_{i=1}^r u^ida_i + \sum_{i=0}^{r-1} (i+1)a_{i+1}u^idu.
  \end{align*}
  Now $df \in \frak p$ if and only if the $i=0$ term in the second sum is zero,
  i.e. if and only $a_1du = 0$.
  However, since $du \neq 0$, we have
  \[ df \in \frak p \iff a_1 = 0 \iff f \in \frak p^2. \]
\end{proof}
\end{comment}
